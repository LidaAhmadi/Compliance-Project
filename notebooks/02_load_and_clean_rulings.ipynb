{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c934a3-abe8-4b62-93c9-3aafc7599a53",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "In this notebook, I will process the raw text files fetched in the previous step. My goal is to clean and structure this data into a `pandas` DataFrame that will serve as the primary input for all subsequent analysis.\n",
    "\n",
    "The process involves:\n",
    "-   Setting up the environment and importing necessary libraries.\n",
    "-   Loading each raw text file.\n",
    "-   Applying custom functions to clean the raw text.\n",
    "-   Extracting the two most critical fields for this project: the core **`decision_text`** and its associated **`party_line`**.\n",
    "-   Saving the final, cleaned DataFrame to the `processed` data folder.\n",
    "\n",
    "While the raw data contains additional metadata (like dates, judges, etc.), I made a strategic decision to defer full metadata extraction to a future phase. This allowed me to keep the scope of Phase 1 focused on building and validating the core text analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f339dffd-7ca8-462f-9687-a70544261a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already at project root: /Users/lidasmac/compliance-nlp\n"
     ]
    }
   ],
   "source": [
    "# --- Foundational Library Imports ---\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Project Root Setup ---\n",
    "# This block ensures the notebook's working directory is always the project root (compliance-nlp/).\n",
    "# This makes all file paths for data and source code consistent and reproducible.\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'notebooks':\n",
    "    os.chdir(current_dir.parent)\n",
    "    print(f\"Changed working directory to project root: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Already at project root: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd45c94a-8e23-48c3-931b-32c79ba9fddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Custom & Configuration Imports ---\n",
    "# This cell can only run correctly after the working directory has been set above.\n",
    "from config import DATA_DIR\n",
    "\n",
    "# Import all the specific cleaning functions from my custom preprocessing module\n",
    "from src.preprocessing import (\n",
    "    clean_html,\n",
    "    extract_decision_section,\n",
    "    clean_and_format_decision,\n",
    "    extract_party_block,\n",
    "    clean_party_line,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c62182-5d4b-438c-974d-7bbce42f87e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Load and Inspect Raw Rulings\n",
    "\n",
    "First, I will load all the raw `.txt` files from the `data/raw` directory into a list. I'll then confirm the number of rulings loaded, to ensure the data has been loaded correctly before proceeding with cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ed8fba0-083d-4c75-a810-5ea16447fbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74 rulings from 'data/raw' into memory.\n"
     ]
    }
   ],
   "source": [
    "# --- Load all raw ruling documents from the target directory ---\n",
    "\n",
    "# 1. Define the directory containing the raw text files\n",
    "# This uses the DATA_DIR from our config and robustly targets the 'raw' sub-directory.\n",
    "raw_data_dir = Path(DATA_DIR) / \"raw\"\n",
    "\n",
    "# 2. Gather all .txt file paths using pathlib's glob method\n",
    "# This is a clean and efficient way to find all matching files.\n",
    "filepaths = sorted(list(raw_data_dir.glob(\"*.txt\")))\n",
    "\n",
    "# 3. Read and store the content of each ruling in a list\n",
    "# This list comprehension is a concise way to build the 'documents' list.\n",
    "documents = [path.read_text(encoding=\"utf-8\").strip() for path in filepaths]\n",
    "\n",
    "# 4. Confirm the number of rulings loaded\n",
    "print(f\"Loaded {len(documents)} rulings from '{raw_data_dir}' into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c85b99-c4c6-40d0-b75a-2f3f9a584d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Preview Raw Rulings\n",
    "\n",
    "Before I begin preprocessing, I'll preview a few raw rulings. This manual inspection helps me understand the text's structure, identify common formatting issues, and plan the specific cleaning functions I'll need to apply.\n",
    "\n",
    "I will start by examining the first two rulings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "523c5355-e06a-47e8-a96f-5ef69ea4add2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ruling 1 Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<div>\n",
      "\n",
      "Bank of N.Y. Mellon v Fenimore St. Realty, Inc. (<span class=\"citation\" data-id=\"10889333\"><a href=\"/opinion/10422745/bank-of-ny-mellon-v-fenimore-st-realty-inc/\" aria-description=\"Citation for case: Bank of N.Y. Mellon v. Fenimore St. Realty, Inc.\">2025 NY Slip Op 02566</a></span>)\n",
      "\n",
      "\n",
      "\n",
      "<table\n",
      "\n",
      " End of Ruling 1 Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " Ruling 2 Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<div>\n",
      "\n",
      "Citimortgage, Inc. v Benyacob (<span class=\"citation\" data-id=\"10889332\"><a href=\"/opinion/10422744/citimortgage-inc-v-benyacob/\" aria-description=\"Citation for case: Citimortgage, Inc. v. Benyacob\">2025 NY Slip Op 02567</a></span>)\n",
      "\n",
      "\n",
      "\n",
      "<table width=\"80%\" border=\"1\" cellspacing=\"2\" cellpadding\n",
      "\n",
      " End of Ruling 2 Preview\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Preview a few raw rulings to inspect Structure ---\n",
    "for i, doc in enumerate(documents[:2]):\n",
    "    print(f\"\\n Ruling {i+1} Preview\\n{'-'*100}\")\n",
    "    print(doc[:300]) \n",
    "    print(f\"\\n End of Ruling {i+1} Preview\\n{'-'*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925ad98-982e-4325-bf5e-a9f331282e88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: Initial Text Cleaning\n",
    "\n",
    "To prepare the text for the next phase of analysis (rule-based risk labeling), I will now apply a lightweight cleaning process. My focus here is on preserving as much of the original document structure as possible.\n",
    "\n",
    "Therefore, my cleaning strategy for this initial phase is very specific:\n",
    "-   I will remove any leftover HTML tags and markup.\n",
    "-   I will ensure line breaks are preserved to maintain paragraph and logical structure.\n",
    "-   I will **not** perform aggressive normalization like lowercasing or stopword removal at this stage. My rule-matching engine will handle case-insensitivity, and preserving the original casing can be useful for manual review.\n",
    "\n",
    "This approach ensures the text is clean enough for processing while maintaining its integrity for accurate rule matching and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c292767e-c356-4585-bfc1-1cabb99779e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML cleaning applied to all 74 documents.\n"
     ]
    }
   ],
   "source": [
    "# --- Apply HTML cleaning to all raw documents ---\n",
    "# I'll use a list comprehension for an efficient and readable way to apply\n",
    "# my custom clean_html function to every document.\n",
    "cleaned_documents = [clean_html(doc) for doc in documents]\n",
    "\n",
    "print(f\"HTML cleaning applied to all {len(cleaned_documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b655e5a-d99f-4524-9de3-804eeba486b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Verification: Compare Before and After Cleaning\n",
    "\n",
    "To confirm the cleaning worked as expected, I will now inspect the \"before\" and \"after\" versions of the first document. This provides a direct, qualitative check of the function's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2a6cc6e-e340-4d9a-afba-73a89d5cac2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning (Raw)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<div>\n",
      "\n",
      "Bank of N.Y. Mellon v Fenimore St. Realty, Inc. (<span class=\"citation\" data-id=\"10889333\"><a href=\"/opinion/10422745/bank-of-ny-mellon-v-fenimore-st-realty-inc/\" aria-description=\"Citation for case: Bank of N.Y. Mellon v. Fenimore St. Realty, Inc.\">2025 NY Slip Op 02566</a></span>)\n",
      "\n",
      "\n",
      "\n",
      "<table width=\"80%\" border=\"1\" cellspacing=\"2\" cellpadding=\"5\" align=\"center\">\n",
      "<tr>\n",
      "<td align=\"center\"><b>Bank of N.Y. Mellon v Fenimore St. Realty, Inc.</b></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td align=\"center\"><span class=\"c...\n",
      "\n",
      "After Cleaning\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bank of N.Y. Mellon v Fenimore St. Realty, Inc. (\n",
      "2025 NY Slip Op 02566\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bank of N.Y. Mellon v Fenimore St. Realty, Inc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2025 NY Slip Op 02566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Decided on April 30, 2025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appellate Division, Second Department\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Published by New York State Law Reporting Bureau pursuant to Judiciary Law § 431.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This opinion is uncorrected and subject to revision before publication in the Official Reports.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Decided on April 30, 2025\n",
      "SUPREME COURT OF THE STATE OF NEW YORK\n",
      "Appe...\n"
     ]
    }
   ],
   "source": [
    "# --- Verification Step ---\n",
    "# I'll compare the raw and cleaned versions of the first document.\n",
    "print(f\"Before Cleaning (Raw)\\n{'-'*100}\")\n",
    "print(documents[0][:500] + \"...\") # Show first 500 characters\n",
    "\n",
    "print(f\"\\nAfter Cleaning\\n{'-'*100}\")\n",
    "print(cleaned_documents[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de27940-3c18-4d84-9286-6aa52386f97b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Save Cleaned Documents for Manual Review\n",
    "\n",
    "As the final step in this initial cleaning phase, I will save each of the 74 cleaned documents to its own text file.\n",
    "\n",
    "Saving the files at this stage allows me to open and manually inspect a sample of them outside of the notebook. This offline review is essential for identifying the consistent patterns and document structure needed for the next phase of metadata extraction.\n",
    "\n",
    "The detailed structural analysis I present in the next step is the direct result of the insights I gained from this manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77245e7e-1bb1-4abd-9724-f9e4dca86f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 74 cleaned documents to the 'data/processed/01_initial_clean' directory.\n"
     ]
    }
   ],
   "source": [
    "# --- Save each cleaned document as a separate .txt file ---\n",
    "\n",
    "# 1. Define the output directory within our established 'processed' folder\n",
    "output_dir = Path(DATA_DIR) / \"processed\"/ \"01_initial_clean\"\n",
    "os.makedirs(output_dir, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "# 2. Loop through the original filepaths and the cleaned documents in parallel\n",
    "#    The zip() function pairs each original path with its corresponding cleaned text.\n",
    "for original_path, cleaned_text in zip(filepaths, cleaned_documents):\n",
    "    \n",
    "    # 3. Construct the new output path, keeping the original filename\n",
    "    #    original_path.name gets the filename (e.g., \"opinion_12345.txt\")\n",
    "    new_filepath = output_dir / original_path.name\n",
    "    \n",
    "    # 4. Save the cleaned text to the new file\n",
    "    with open(new_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text)\n",
    "\n",
    "print(f\"Successfully saved {len(cleaned_documents)} cleaned documents to the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b194cf8-2f62-4578-913e-f666eb5c01ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5: Analyze Ruling Structure & Plan for Extraction\n",
    "\n",
    "Now that the documents are clean, my next goal is to extract structured information from them, specifically the `party_line` and the core `decision_text`.\n",
    "\n",
    "Based on my manual review of the cleaned documents, I've identified a consistent structure that can be used for parsing. Understanding this schema is the critical first step before writing the extraction functions.\n",
    "\n",
    "### Identified Ruling Structure\n",
    "\n",
    "Each court ruling generally follows this format:\n",
    "\n",
    "**1. Case Header**\n",
    "* Case name (e.g., *People v Palm*)\n",
    "* Slip opinion number and decision date\n",
    "* Court (e.g., Appellate Division, Second Department)\n",
    "\n",
    "**2. Party and Counsel Information**\n",
    "* Named parties: appellant(s) and respondent(s)\n",
    "* Legal representation: attorney listings for each side\n",
    "\n",
    "**3. Decision Section**\n",
    "* Procedural Summary: What is being appealed and why.\n",
    "* Outcome Statement: Often begins with `ORDERED that...`.\n",
    "* Factual Background & Legal Analysis: The core reasoning of the court.\n",
    "\n",
    "**4. Conclusion**\n",
    "* Final judgment (affirmed/reversed/remanded).\n",
    "* List of concurring judges.\n",
    "\n",
    "This structural analysis provides the roadmap for the functions I will now apply to extract the key data fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190f8a5-4c93-427f-a2ea-24143e6896ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6: Extract the Core Decision Text\n",
    "\n",
    "Now that I understand the document structure, I will extract the core decision section from each ruling. This part of the text contains the legal analysis and conclusions, which are essential for our risk and relevance assessments.\n",
    "\n",
    "I locate the decision body using the following logic:\n",
    "-   **Start** the extraction *after* a common heading like `\"DECISION & ORDER\"`, if one is present.\n",
    "-   **Stop** the extraction *before* common sign-offs like judge signatures (e.g., lines containing `J.P.` or `JJ.`) or official closings like `\"ENTERED:\"`.\n",
    "\n",
    "This approach ensures a clean, focused extraction of the ruling's reasoning without including header metadata or footer signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1d538b2-7a7c-464b-9b95-44b8d7d42b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core decision text extracted from all 74 documents.\n"
     ]
    }
   ],
   "source": [
    "# --- Extract decision sections from all cleaned rulings ---\n",
    "# I'll apply my custom extract_decision_section function to the list of cleaned documents.\n",
    "decision_texts = [extract_decision_section(doc) for doc in cleaned_documents]\n",
    "\n",
    "print(f\"Core decision text extracted from all {len(decision_texts)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b7f3b-a8cb-4931-bcc4-2f6f667c33e6",
   "metadata": {},
   "source": [
    "Now, I will preview the first two extracted decision sections to verify that the parsing logic worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5445c32-b04d-45eb-96c1-04e38243d2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 Decision Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In an action to foreclose a mortgage, the defendant Fenimore St. Realty, Inc., appeals from an order of the Supreme Court, Kings County (Larry D. Martin, J.), dated April 25, 2023. The order, insofar as appealed from, denied that defendant's cross-motion to reduce the amount of postjudgment interest accrued.\n",
      "ORDERED that the order is affirmed insofar as appealed from, with costs.\n",
      "In 2011, the plaintiff commenced this action to foreclose a mortgage on certain real property located in Brooklyn. In...\n",
      "End of Doc 1 Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Doc 2 Decision Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In an action to foreclose a mortgage, the defendant Yehudit Benyacob appeals from an order of the Supreme Court, Kings County (Larry D. Martin, J.), dated March 29, 2023. The order, insofar as appealed from, denied that defendant's motion for summary judgment dismissing the complaint insofar as asserted against her for lack of personal jurisdiction and granted that branch of the plaintiff's cross-motion which was pursuant to CPLR 306-b to extend the time to serve that defendant with the summons ...\n",
      "End of Doc 2 Preview\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Preview a few extracted decision sections ---\n",
    "for i, decision in enumerate(decision_texts[:2]):\n",
    "    print(f\"Doc {i+1} Decision Preview\\n{'-'*100}\")\n",
    "    \n",
    "    # This check handles cases where no decision text could be found\n",
    "    if decision:\n",
    "        print(decision[:500] + \"...\")\n",
    "    else:\n",
    "        print(\"--> No decision text found for this document.\")\n",
    "        \n",
    "    print(f\"End of Doc {i+1} Preview\\n{'-'*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03001c-047a-4439-9e52-e02aaff8c992",
   "metadata": {},
   "source": [
    "### 6.1 Identify and Review Missing Decisions\n",
    "\n",
    "After attempting to extract the decision section from each ruling, I will now check for any documents where this process failed (i.e., returned an empty result).\n",
    "\n",
    "This is a critical data quality step that helps me:\n",
    "-   Quantify how many rulings may be unusable for future analysis.\n",
    "-   Identify potential edge cases or inputs that my parsing logic missed.\n",
    "-   Manually inspect these problematic cases to decide whether to exclude them or refine my extraction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d93259a-97a3-4e43-837b-ad9f897e4baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 documents where decision text could not be extracted.\n",
      "   Failed indices: [10, 29, 38, 41, 43, 44, 45, 46, 47, 50, 51, 53, 54, 60, 68]\n",
      "   Saving these documents to 'data/processed/01_decision_parsing_failures' for review ...\n",
      "\n",
      "Process complete. Saved 15 failed documents.\n"
     ]
    }
   ],
   "source": [
    "# --- Identify, Inspect, and Save Rulings with Missing Decision Sections ---\n",
    "\n",
    "# 1. Define and create a dedicated folder for any parsing failures\n",
    "failure_dir = Path(DATA_DIR) / \"processed\" / \"01_decision_parsing_failures\"\n",
    "os.makedirs(failure_dir, exist_ok=True)\n",
    "\n",
    "# 2. Use a list comprehension to efficiently find the indices of all failed documents\n",
    "missing_indices = [i for i, text in enumerate(decision_texts) if not text]\n",
    "\n",
    "# 3. Report the summary and save the failed files if any exist\n",
    "if missing_indices:\n",
    "    print(f\"Found {len(missing_indices)} documents where decision text could not be extracted.\")\n",
    "    print(f\"   Failed indices: {missing_indices}\")\n",
    "    print(f\"   Saving these documents to '{failure_dir}' for review ...\")\n",
    "\n",
    "    # Loop through the failed indices and save each corresponding document\n",
    "    for i in missing_indices:\n",
    "        # Get the original filename to preserve the link to the raw data\n",
    "        original_filename = filepaths[i].name\n",
    "        output_path = failure_dir / original_filename\n",
    "        \n",
    "        # Save the full cleaned document that caused the failure\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned_documents[i])\n",
    "            \n",
    "    print(f\"\\nProcess complete. Saved {len(missing_indices)} failed documents.\")\n",
    "else:\n",
    "    print(\"Process complete. All documents were parsed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fea6d8-fe93-4882-8c94-88ad856b5aa0",
   "metadata": {},
   "source": [
    "### 6.2 Summary of Extraction Results & Path Forward\n",
    "\n",
    "My analysis of the 15 documents that failed parsing reveals a clear pattern. The failures are primarily due to variations in the document headers that my initial strategy did not account for.\n",
    "\n",
    "Common headers in the failed documents include:\n",
    "-   `Decision and Judgement`\n",
    "-   `Opinion and Order`\n",
    "-   `Order, Supreme Court`\n",
    "-   `Order, Family Court`\n",
    "-   `Order of fact-finding`\n",
    "\n",
    "While I could refine my `extract_decision_section` function to handle these new patterns, the current process has already successfully extracted **59 high-quality decision texts.** This is a more than sufficient dataset for the primary goal of this project phase: to build, refine, and evaluate the rule-based Triage model.\n",
    "\n",
    "Therefore, to maintain project momentum and keep the scope focused, **I will proceed with the 59 successfully parsed documents.** Improving the parser's robustness to handle these additional header formats is a valuable task that I will note for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d697ed-51d6-468f-81a2-9e8f9fe39df5",
   "metadata": {},
   "source": [
    "## Step 7: Final Text Cleaning and Formatting\n",
    "\n",
    "As the final preprocessing step in this notebook, I will apply a more detailed cleaning and formatting routine to the extracted decision texts. My goal is to create a clean, consistently structured text body that is ready for the next steps of the analysis.\n",
    "\n",
    "My two-step process is as follows:\n",
    "\n",
    "**1. Text Cleaning:**\n",
    "* I will remove editorial markers like `[*1]`.\n",
    "* I will collapse multiple line breaks and normalize all whitespace.\n",
    "* I will preserve legal citations (e.g., *People v De Bour, 40 NY2d 210*), as they are important parts of the text.\n",
    "\n",
    "**2. Readability Formatting:**\n",
    "* To improve readability and prepare the text for later steps, I will insert paragraph breaks after key transitional phrases such as `\"ORDERED that\"`, `\"Here,\"`, and `\"Accordingly,\"`.\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "This structured cleaning is important for the project's future phases:\n",
    "-   **For Human Review:** It makes the text much easier for an analyst to read.\n",
    "-   **For Semantic Search:** It helps break the text into more logical chunks.\n",
    "-   **For LLMs:** A clean, well-structured input improves the quality of summarization and other language model tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbe536b9-5047-48da-8693-827e3e75e0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaning and formatting applied to 59 documents.\n"
     ]
    }
   ],
   "source": [
    "# --- Apply final cleaning and formatting to the extracted decision texts ---\n",
    "# I'll use a list comprehension to apply the function, filtering out any\n",
    "# documents where the initial extraction failed.\n",
    "formatted_texts = [clean_and_format_decision(t) for t in decision_texts if t] # 'if t' is a concise way to check for not None and not empty\n",
    "\n",
    "print(f\"Final cleaning and formatting applied to {len(formatted_texts)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea2ea6-65fe-4f3d-8d3a-531618eb162b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, I will preview the first two formatted texts to confirm that my final cleaning and formatting logic worked as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d05cd5b4-52f2-414e-8fe7-8e571cdb0b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted Ruling 1 Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In an action to foreclose a mortgage, the defendant Fenimore St. Realty, Inc., appeals from an order of the Supreme Court, Kings County (Larry D. Martin, J.), dated April 25, 2023. The order, insofar as appealed from, denied that defendant's cross-motion to reduce the amount of postjudgment interest accrued. \n",
      "\n",
      "ORDERED that the order is affirmed insofar as appealed from, with costs. In 2011, the plaintiff commenced this action to foreclose a mortgage on certain real property located in Brooklyn. ...\n",
      "\n",
      "End of Formatted Ruling 1 Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Formatted Ruling 2 Preview\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In an action to foreclose a mortgage, the defendant Yehudit Benyacob appeals from an order of the Supreme Court, Kings County (Larry D. Martin, J.), dated March 29, 2023. The order, insofar as appealed from, denied that defendant's motion for summary judgment dismissing the complaint insofar as asserted against her for lack of personal jurisdiction and granted that branch of the plaintiff's cross-motion which was pursuant to CPLR 306-b to extend the time to serve that defendant with the summons ...\n",
      "\n",
      "End of Formatted Ruling 2 Preview\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Preview the first two fully processed rulings ---\n",
    "for i, text in enumerate(formatted_texts[:2]):\n",
    "    print(f\"\\nFormatted Ruling {i+1} Preview\\n{'-'*100}\")\n",
    "    # Previewing a slice from the middle of the text can be a good way to see the formatting\n",
    "    print(text[:500] + \"...\") \n",
    "    print(f\"\\nEnd of Formatted Ruling {i+1} Preview\\n{'-'*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76179754-22c0-4d20-9c2e-514b391fd012",
   "metadata": {},
   "source": [
    "## Step 8: Extract Party Information\n",
    "\n",
    "Next, I will extract the named parties from each case to use for metadata tagging and filtering in later analysis.\n",
    "\n",
    "### My Extraction Logic\n",
    "\n",
    "Based on my review of the document structure, I will use a regular expression to isolate the party information:\n",
    "-   First, I locate the block of text that starts with the `[*1]` marker and ends just before a major section heading like `\"DECISION & ORDER\"`.\n",
    "-   Within that block, I clean up the lines and extract the relevant party names.\n",
    "-   Finally, I combine these lines into a single `party_line` string (e.g., *Party A v Party B*).\n",
    "\n",
    "### Why This is Valuable\n",
    "\n",
    "Extracting this structured information allows me to:\n",
    "\n",
    "-   Index or group rulings by case participants (e.g., to find recurring litigants).\n",
    "-   Enable a more user-friendly display of results in dashboards or search tools.\n",
    "\n",
    "While other metadata (like judge names or docket numbers) could also be extracted, I am currently prioritizing the decision content and party identity to keep the scope of this phase focused. These other fields can be layered in later as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7de469c9-6291-4b2c-ae6c-4b54a5ec0874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Party information extracted from all 74 documents.\n"
     ]
    }
   ],
   "source": [
    "# --- Apply party extraction to all cleaned documents ---\n",
    "# I'll use a list comprehension for a concise way to apply my custom function.\n",
    "# The results will be stored as a list of tuples: (original_index, extracted_party_string).\n",
    "extracted_parties = [\n",
    "    (i, extract_party_block(doc)) for i, doc in enumerate(cleaned_documents)\n",
    "]\n",
    "\n",
    "print(f\"Party information extracted from all {len(extracted_parties)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3dee2-9d72-46f6-b4ed-1a04bde82400",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, I will preview the first 10 extracted party lines to verify that the parsing logic worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a031ccc-b8c2-45af-bfa5-b3abe5e93332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample of Extracted Party Information ---\n",
      "Doc 0: 'Bank of New York Mellon, etc., respondent, v Fenimore St. Realty, Inc., appellant, et al., defendants.'\n",
      "Doc 1: 'Citimortgage, Inc., etc., respondent, v Yehudit Benyacob, etc., appellant, et al., defendants. Henry Kohn, Brooklyn, NY, for appellant.'\n",
      "Doc 2: 'In the Matter of 853-855 McLean, LLC, respondent, v City of Yonkers, NY, et al., appellants.'\n",
      "Doc 3: 'Edwin Delcid-Funez, appellant-respondent, v Seasons at East Meadow Home Owners Association, Inc., et al., respondents-appellants, et al., defendant.'\n",
      "Doc 4: 'In the Matter of Kristin M. Cirillo, appellant, v Ronald Grullon, respondent. Kenneth M. Tuccillo, Hastings on Hudson, NY, for appellant. Steven A. Feldman, Manhasset, NY, for respondent.'\n",
      "Doc 5: 'P. A., respondent, v Poly Prep Country Day School, appellant.'\n",
      "Doc 6: 'Maria Airene Pantanilla, respondent, v Guillerma Yuson, appellant. David De Andrade, New York, NY, for appellant.'\n",
      "Doc 7: 'The People of the State of New York, respondent, v Brendan Dowling, appellant.'\n",
      "Doc 8: 'In the Matter of Clifton C. (Anonymous), petitioner-appellant, v Tory P. R. (Anonymous), respondent-respondent; Jayia R. (Anonymous), nonparty-appellant. Salvatore C. Adamo, New York, NY, for petitioner-appellant. Mark A. Peterson, Smithtown, NY, attorney for the child, the nonparty-appellant. Arza Feldman, Manhasset, NY, for respondent-respondent.'\n",
      "Doc 9: 'In the Matter of Pamela De Phillips, etc., appellant, v Nicole Pascone Perez, respondent. Diana Kelly, Jamaica, NY, for appellant. Deana Balahtsis, New York, NY, for respondent.'\n"
     ]
    }
   ],
   "source": [
    "# --- Preview the first 10 extracted party lines ---\n",
    "print(\"--- Sample of Extracted Party Information ---\")\n",
    "\n",
    "for i, party_line in extracted_parties[:10]:\n",
    "    # This check makes the output clearer if the function failed for a document\n",
    "    if party_line:\n",
    "        print(f\"Doc {i}: '{party_line}'\")\n",
    "    else:\n",
    "        print(f\"Doc {i}: --> No party information found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15df7c2-52b5-47fb-becd-fa3019c3b505",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.1 Final Party Line Cleanup\n",
    "\n",
    "As a final refinement step, I will clean the extracted party lines by removing common legal prefixes like `\"In the Matter of\"`.\n",
    "\n",
    "This standardization is important for creating a clean and consistent `party_line` column in the final DataFrame I will build next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9b2bfe7-3483-40c9-92d1-83de23aefc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleanup applied to all 74 party lines.\n"
     ]
    }
   ],
   "source": [
    "# --- Apply final cleanup to the extracted party lines ---\n",
    "# I'll use a list comprehension to apply my custom clean_party_line function.\n",
    "cleaned_parties = [(i, clean_party_line(line)) for i, line in extracted_parties]\n",
    "\n",
    "print(f\"Final cleanup applied to all {len(cleaned_parties)} party lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9343cb5-8bd4-4849-b058-9bf9501a3667",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, I will preview a sample of the cleaned party lines to verify that the prefixes were removed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b4b80ec-5598-4979-962d-8bc179a45a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample of Final, Cleaned Party Information ---\n",
      "Doc 0: 'Bank of New York Mellon, etc., respondent, v Fenimore St. Realty, Inc., appellant, et al., defendants.'\n",
      "Doc 1: 'Citimortgage, Inc., etc., respondent, v Yehudit Benyacob, etc., appellant, et al., defendants. Henry Kohn, Brooklyn, NY, for appellant.'\n",
      "Doc 2: '853-855 McLean, LLC, respondent, v City of Yonkers, NY, et al., appellants.'\n",
      "Doc 3: 'Edwin Delcid-Funez, appellant-respondent, v Seasons at East Meadow Home Owners Association, Inc., et al., respondents-appellants, et al., defendant.'\n",
      "Doc 4: 'Kristin M. Cirillo, appellant, v Ronald Grullon, respondent. Kenneth M. Tuccillo, Hastings on Hudson, NY, for appellant. Steven A. Feldman, Manhasset, NY, for respondent.'\n",
      "Doc 5: 'P. A., respondent, v Poly Prep Country Day School, appellant.'\n",
      "Doc 6: 'Maria Airene Pantanilla, respondent, v Guillerma Yuson, appellant. David De Andrade, New York, NY, for appellant.'\n",
      "Doc 7: 'The People of the State of New York, respondent, v Brendan Dowling, appellant.'\n",
      "Doc 8: 'Clifton C. (Anonymous), petitioner-appellant, v Tory P. R. (Anonymous), respondent-respondent; Jayia R. (Anonymous), nonparty-appellant. Salvatore C. Adamo, New York, NY, for petitioner-appellant. Mark A. Peterson, Smithtown, NY, attorney for the child, the nonparty-appellant. Arza Feldman, Manhasset, NY, for respondent-respondent.'\n",
      "Doc 9: 'Pamela De Phillips, etc., appellant, v Nicole Pascone Perez, respondent. Diana Kelly, Jamaica, NY, for appellant. Deana Balahtsis, New York, NY, for respondent.'\n"
     ]
    }
   ],
   "source": [
    "# --- Preview the first 10 cleaned party lines ---\n",
    "print(\"--- Sample of Final, Cleaned Party Information ---\")\n",
    "\n",
    "for i, party_line in cleaned_parties[:10]: # We'll just preview the first 10\n",
    "    if party_line:\n",
    "        print(f\"Doc {i}: '{party_line}'\")\n",
    "    else:\n",
    "        # This handles cases where the original extraction might have failed\n",
    "        print(f\"Doc {i}: --> No party information found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7db5b2-526e-425c-92de-6b0b52a67cc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 9: Create and Save the Final DataFrame\n",
    "\n",
    "As the final step in this notebook, I will combine all the cleaned and extracted pieces of information—the party lines and the formatted decision texts—into a single, structured `pandas` DataFrame.\n",
    "\n",
    "A key decision here is how to handle the 15 documents for which my parsing logic could not extract a decision text. For data integrity and traceability, **I will keep all 74 original rows** in the final DataFrame.\n",
    "\n",
    "The `decision_text` for the failed documents will be stored as an empty string. This maintains a perfect 1-to-1 mapping back to the raw source files and allows for easy filtering in subsequent analysis.\n",
    "\n",
    "This final DataFrame is the key deliverable from this preprocessing pipeline and will be the primary input for the next phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cfd4f31-c962-487b-aa9d-4f5b13183973",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created DataFrame with 74 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>party_line</th>\n",
       "      <th>decision_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bank of New York Mellon, etc., respondent, v F...</td>\n",
       "      <td>In an action to foreclose a mortgage, the defe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Citimortgage, Inc., etc., respondent, v Yehudi...</td>\n",
       "      <td>In an action to foreclose a mortgage, the defe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>853-855 McLean, LLC, respondent, v City of Yon...</td>\n",
       "      <td>In a proceeding pursuant to CPLR article 78 to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Edwin Delcid-Funez, appellant-respondent, v Se...</td>\n",
       "      <td>In an action to recover damages for personal i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Kristin M. Cirillo, appellant, v Ronald Grullo...</td>\n",
       "      <td>In a proceeding pursuant to Family Court Act a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_index                                         party_line  \\\n",
       "0          0  Bank of New York Mellon, etc., respondent, v F...   \n",
       "1          1  Citimortgage, Inc., etc., respondent, v Yehudi...   \n",
       "2          2  853-855 McLean, LLC, respondent, v City of Yon...   \n",
       "3          3  Edwin Delcid-Funez, appellant-respondent, v Se...   \n",
       "4          4  Kristin M. Cirillo, appellant, v Ronald Grullo...   \n",
       "\n",
       "                                       decision_text  \n",
       "0  In an action to foreclose a mortgage, the defe...  \n",
       "1  In an action to foreclose a mortgage, the defe...  \n",
       "2  In a proceeding pursuant to CPLR article 78 to...  \n",
       "3  In an action to recover damages for personal i...  \n",
       "4  In a proceeding pursuant to Family Court Act a...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying a row (index 10) where parsing failed ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>party_line</th>\n",
       "      <th>decision_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Angel I. Rodriguez, petitioner, v New York Sta...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_index                                         party_line decision_text\n",
       "10         10  Angel I. Rodriguez, petitioner, v New York Sta...              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Create the Final DataFrame ---\n",
    "\n",
    "# 1. Prepare the 'party_line' data\n",
    "# I'll extract just the text from the 'cleaned_parties' list of tuples.\n",
    "final_party_lines = [line for i, line in cleaned_parties]\n",
    "\n",
    "# 2. Prepare the 'decision_text' data\n",
    "# I'll use the original 'decision_texts' list (which has 74 items) and\n",
    "# replace any 'None' values with an empty string to ensure data integrity.\n",
    "final_decision_texts = [text if text is not None else \"\" for text in decision_texts]\n",
    "\n",
    "# 3. Create the DataFrame from a dictionary of these lists\n",
    "# This is the standard, efficient way to build a DataFrame and ensures all\n",
    "# columns have the same length (74).\n",
    "metadata_df = pd.DataFrame({\n",
    "    \"doc_index\": range(len(documents)), # Create an index from 0 to 73\n",
    "    \"party_line\": final_party_lines,\n",
    "    \"decision_text\": final_decision_texts\n",
    "})\n",
    "\n",
    "# 4. Verification\n",
    "print(f\"Successfully created DataFrame with {len(metadata_df)} rows.\")\n",
    "display(metadata_df.head())\n",
    "\n",
    "# To be certain, I'll also check a row where the decision text was missing\n",
    "# to confirm it was handled correctly as an empty string.\n",
    "try:\n",
    "    first_empty_index = metadata_df[metadata_df['decision_text'] == \"\"].index[0]\n",
    "    print(f\"\\n--- Verifying a row (index {first_empty_index}) where parsing failed ---\")\n",
    "    display(metadata_df.iloc[first_empty_index:first_empty_index+1])\n",
    "except IndexError:\n",
    "    print(\"\\n--- No rows with empty decision text found. All documents parsed successfully. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a1ec5-2500-4454-8841-cd2e6d501a05",
   "metadata": {},
   "source": [
    "## Step 10: Save the Final Cleaned Dataset\n",
    "\n",
    "As the final action in this notebook, I will save the cleaned and structured `metadata_df` to a CSV file.\n",
    "\n",
    "This file will be stored in the `data/processed/` directory and will serve as the primary input for the next notebook in our pipeline, where I will perform the rule-based labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90a15ed1-1971-4dc0-b1e1-486844911ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame successfully saved to: data/processed/party_and_decision_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Define the output path and save the DataFrame ---\n",
    "\n",
    "# 1. Define the output directory using our established structure\n",
    "output_dir = Path(DATA_DIR) / \"processed\"\n",
    "os.makedirs(output_dir, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "# 2. Construct the full file path\n",
    "output_path = output_dir / \"party_and_decision_metadata.csv\"\n",
    "\n",
    "# 3. Save the DataFrame to CSV, excluding the pandas index\n",
    "metadata_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Final DataFrame successfully saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Compliance Project)",
   "language": "python",
   "name": "compliance_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
